#!/bin/bash

# `infile` should contain a list with the file names in hdfs you want
# processed. One name each line. See
# http://hadoop.apache.org/docs/r1.0.3/streaming.html#Making+Files+Available+to+Tasks
# for further information on how to combine this with the `#` in the
# `FILES` variable.
USER_HADOOP="hdfs://localhost:9000/user/${USER}"
INPUT="${USER_HADOOP}/input/infile"
ALEXA_FILES="${USER_HADOOP}/alexa-files#data"

# Where the output should go. THIS IS GOING TO BE DELETED FURTHER
# DOWN!
OUT="name_count"

# Set this according to your environment
HADOOP_HOME="${HOME}/opt/hadoop-1.0.3"

# Set to a value which best fits your environment.
NUM_MAPS="16"
NUM_REDUCERS="1"

MAPPER="$(which md_alexa_to_names_and_one)"
REDUCER="$(which md_sum_values)"
HADOOP_BIN="${HADOOP_HOME}/bin/hadoop"
HADOOP_STREAMING="${HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar"

${HADOOP_BIN} jar ${HADOOP_STREAMING} \
    -D mapred.map.tasks=${NUM_MAPS} \
    -D mapred.reduce.tasks=${NUM_REDUCERS} \
    -D mapred.output.compress=true \
    -D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec \
    -files ${ALEXA_FILES} \
    -input ${INPUT} \
    -output "${OUT}" \
    -mapper "${MAPPER}" \
    -reducer "${REDUCER}"
