#!/bin/bash

# `infile` should contain a list with the file names in hdfs you want
# processed. One name each line. See
# http://hadoop.apache.org/docs/r1.0.3/streaming.html#Making+Files+Available+to+Tasks
# for further information on how to combine this with the `#` in the
# `FILES` variable.
INPUT="input/infile"
FILES="hdfs://localhost:9000/user/thbach/alexa-files#data"

# Where the output should be saved. THIS IS GOING TO BE DELETED
# FURTHER DOWN!
OUT="key-files"

# Set this according to your environment
HADOOP_HOME="${HOME}/opt/hadoop-1.0.3"

# Set to a value which best fits your environment.
NUM_MAPS="16"
NUM_REDUCERS="28"

MAPPER="$(which md_alexa_to_key)"
REDUCER="$(which md_key_to_files) -d ${OUT}"
HADOOP_BIN="${HADOOP_HOME}/bin/hadoop"
HADOOP_STREAMING="${HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar"

# Delete the output directory
${HADOOP_BIN} fs -rmr ${OUT}


${HADOOP_BIN} jar ${HADOOP_STREAMING} \
    -D mapred.map.tasks=${NUM_MAPS} \
    -D mapred.reduce.tasks=${NUM_REDUCERS} \
    -files ${FILES} \
    -input ${INPUT} \
    -output "${OUT}#${OUT}" \
    -mapper "${MAPPER}" \
    -reducer "${REDUCER}"
