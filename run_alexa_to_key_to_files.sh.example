#!/bin/bash

# `infile` should contain a list with the file names in hdfs you want
# processed. One name each line. See
# http://hadoop.apache.org/docs/r1.0.3/streaming.html#Making+Files+Available+to+Tasks
# for further information on how to combine this with the `#` in the
# `FILES` variable.
USER_HADOOP="hdfs://localhost:9000/user/${USER}"
INPUT="${USER_HADOOP}/input/infile"
ALEXA_FILES="${USER_HADOOP}/alexa-files#data"
SHA1_DEST="sha1"
SHA1_FILES="${USER_HADOOP}/sha1-files#${SHA1_DEST}"

# Where the output should go. THIS IS GOING TO BE DELETED FURTHER
# DOWN!
OUT="out"

# Set this according to your environment
HADOOP_HOME="${HOME}/opt/hadoop-1.0.3"

# Set to a value which best fits your environment.
NUM_MAPS="16"
NUM_REDUCERS="28"

MAPPER="$(which md_alexa_to_key)"
REDUCER="$(which md_key_to_files) -d ${SHA1_DEST}"
HADOOP_BIN="${HADOOP_HOME}/bin/hadoop"
HADOOP_STREAMING="${HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar"

# `hadoop' is called in `md_key_to_files'
export PATH="${PATH}:${HADOOP_HOME}/bin"

# Delete the output directory
${HADOOP_BIN} fs -rmr ${OUT}


${HADOOP_BIN} jar ${HADOOP_STREAMING} \
    -D mapred.map.tasks=${NUM_MAPS} \
    -D mapred.reduce.tasks=${NUM_REDUCERS} \
    -files ${ALEXA_FILES} \
    -files ${SHA1_FILES} \
    -input ${INPUT} \
    -output "${OUT}" \
    -mapper "${MAPPER}" \
    -reducer "${REDUCER}"
